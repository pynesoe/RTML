{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RTML Midterm 2022\n",
    "\n",
    "## Question 1 (20 points)\n",
    "\n",
    "In Labs 04 and 05, you developed your own PyTorch implementations of YOLOv4 and YOLOR.\n",
    "Download the image at http://www.cs.ait.ac.th/~mdailey/ait-orientation.jpg and run it\n",
    "through your YOLOv4 and YOLOR models. Provide your source code to load the model, image,\n",
    "get the result, and display the result here. Display the resulting bounding boxes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py\n",
    "from __future__ import division\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import cv2 \n",
    "\n",
    "def unique(tensor):\n",
    "    tensor_np = tensor.cpu().numpy()\n",
    "    unique_np = np.unique(tensor_np)\n",
    "    unique_tensor = torch.from_numpy(unique_np)\n",
    "    \n",
    "    tensor_res = tensor.new(unique_tensor.shape)\n",
    "    tensor_res.copy_(unique_tensor)\n",
    "    return tensor_res\n",
    "\n",
    "\n",
    "def bbox_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Returns the IoU of two bounding boxes \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    #Get the coordinates of bounding boxes\n",
    "    b1_x1, b1_y1, b1_x2, b1_y2 = box1[:,0], box1[:,1], box1[:,2], box1[:,3]\n",
    "    b2_x1, b2_y1, b2_x2, b2_y2 = box2[:,0], box2[:,1], box2[:,2], box2[:,3]\n",
    "    \n",
    "    #get the corrdinates of the intersection rectangle\n",
    "    inter_rect_x1 =  torch.max(b1_x1, b2_x1)\n",
    "    inter_rect_y1 =  torch.max(b1_y1, b2_y1)\n",
    "    inter_rect_x2 =  torch.min(b1_x2, b2_x2)\n",
    "    inter_rect_y2 =  torch.min(b1_y2, b2_y2)\n",
    "    \n",
    "    #Intersection area\n",
    "    inter_area = torch.clamp(inter_rect_x2 - inter_rect_x1 + 1, min=0) * torch.clamp(inter_rect_y2 - inter_rect_y1 + 1, min=0)\n",
    "\n",
    "    #Union Area\n",
    "    b1_area = (b1_x2 - b1_x1 + 1)*(b1_y2 - b1_y1 + 1)\n",
    "    b2_area = (b2_x2 - b2_x1 + 1)*(b2_y2 - b2_y1 + 1)\n",
    "    \n",
    "    iou = inter_area / (b1_area + b2_area - inter_area)\n",
    "    \n",
    "    return iou\n",
    "\n",
    "def predict_transform(prediction, inp_dim, anchors, num_classes, CUDA = True):\n",
    "\n",
    "    \n",
    "    batch_size = prediction.size(0)\n",
    "    stride =  inp_dim // prediction.size(2)\n",
    "    grid_size = inp_dim // stride\n",
    "    bbox_attrs = 5 + num_classes\n",
    "    num_anchors = len(anchors)\n",
    "    \n",
    "    prediction = prediction.view(batch_size, bbox_attrs*num_anchors, grid_size*grid_size)\n",
    "    prediction = prediction.transpose(1,2).contiguous()\n",
    "    prediction = prediction.view(batch_size, grid_size*grid_size*num_anchors, bbox_attrs)\n",
    "    anchors = [(a[0]/stride, a[1]/stride) for a in anchors]\n",
    "\n",
    "    #Sigmoid the  centre_X, centre_Y. and object confidencce\n",
    "    prediction[:,:,0] = torch.sigmoid(prediction[:,:,0])\n",
    "    prediction[:,:,1] = torch.sigmoid(prediction[:,:,1])\n",
    "    prediction[:,:,4] = torch.sigmoid(prediction[:,:,4])\n",
    "    \n",
    "    #Add the center offsets\n",
    "    grid = np.arange(grid_size)\n",
    "    a,b = np.meshgrid(grid, grid)\n",
    "\n",
    "    x_offset = torch.FloatTensor(a).view(-1,1)\n",
    "    y_offset = torch.FloatTensor(b).view(-1,1)\n",
    "\n",
    "    if CUDA:\n",
    "        x_offset = x_offset.cuda()\n",
    "        y_offset = y_offset.cuda()\n",
    "\n",
    "    x_y_offset = torch.cat((x_offset, y_offset), 1).repeat(1,num_anchors).view(-1,2).unsqueeze(0)\n",
    "\n",
    "    prediction[:,:,:2] += x_y_offset\n",
    "\n",
    "    #log space transform height and the width\n",
    "    anchors = torch.FloatTensor(anchors)\n",
    "\n",
    "    if CUDA:\n",
    "        anchors = anchors.cuda()\n",
    "\n",
    "    anchors = anchors.repeat(grid_size*grid_size, 1).unsqueeze(0)\n",
    "    prediction[:,:,2:4] = torch.exp(prediction[:,:,2:4])*anchors\n",
    "    \n",
    "    prediction[:,:,5: 5 + num_classes] = torch.sigmoid((prediction[:,:, 5 : 5 + num_classes]))\n",
    "\n",
    "    prediction[:,:,:4] *= stride\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "def write_results(prediction, confidence, num_classes, nms_conf = 0.4):\n",
    "    conf_mask = (prediction[:,:,4] > confidence).float().unsqueeze(2)\n",
    "    prediction = prediction*conf_mask\n",
    "    \n",
    "    box_corner = prediction.new(prediction.shape)\n",
    "    box_corner[:,:,0] = (prediction[:,:,0] - prediction[:,:,2]/2)\n",
    "    box_corner[:,:,1] = (prediction[:,:,1] - prediction[:,:,3]/2)\n",
    "    box_corner[:,:,2] = (prediction[:,:,0] + prediction[:,:,2]/2) \n",
    "    box_corner[:,:,3] = (prediction[:,:,1] + prediction[:,:,3]/2)\n",
    "    prediction[:,:,:4] = box_corner[:,:,:4]\n",
    "    \n",
    "    batch_size = prediction.size(0)\n",
    "\n",
    "    write = False\n",
    "    \n",
    "\n",
    "\n",
    "    for ind in range(batch_size):\n",
    "        image_pred = prediction[ind]          #image Tensor\n",
    "       #confidence threshholding \n",
    "       #NMS\n",
    "    \n",
    "        max_conf, max_conf_score = torch.max(image_pred[:,5:5+ num_classes], 1)\n",
    "        max_conf = max_conf.float().unsqueeze(1)\n",
    "        max_conf_score = max_conf_score.float().unsqueeze(1)\n",
    "        seq = (image_pred[:,:5], max_conf, max_conf_score)\n",
    "        image_pred = torch.cat(seq, 1)\n",
    "        \n",
    "        non_zero_ind =  (torch.nonzero(image_pred[:,4]))\n",
    "        try:\n",
    "            image_pred_ = image_pred[non_zero_ind.squeeze(),:].view(-1,7)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        if image_pred_.shape[0] == 0:\n",
    "            continue       \n",
    "#        \n",
    "  \n",
    "        #Get the various classes detected in the image\n",
    "        img_classes = unique(image_pred_[:,-1])  # -1 index holds the class index\n",
    "        \n",
    "        \n",
    "        for cls in img_classes:\n",
    "            #perform NMS\n",
    "\n",
    "        \n",
    "            #get the detections with one particular class\n",
    "            cls_mask = image_pred_*(image_pred_[:,-1] == cls).float().unsqueeze(1)\n",
    "            class_mask_ind = torch.nonzero(cls_mask[:,-2]).squeeze()\n",
    "            image_pred_class = image_pred_[class_mask_ind].view(-1,7)\n",
    "            \n",
    "            #sort the detections such that the entry with the maximum objectness\n",
    "            #confidence is at the top\n",
    "            conf_sort_index = torch.sort(image_pred_class[:,4], descending = True )[1]\n",
    "            image_pred_class = image_pred_class[conf_sort_index]\n",
    "            idx = image_pred_class.size(0)   #Number of detections\n",
    "            \n",
    "            for i in range(idx):\n",
    "                #Get the IOUs of all boxes that come after the one we are looking at \n",
    "                #in the loop\n",
    "                try:\n",
    "                    ious = bbox_iou(image_pred_class[i].unsqueeze(0), image_pred_class[i+1:])\n",
    "                except ValueError:\n",
    "                    break\n",
    "            \n",
    "                except IndexError:\n",
    "                    break\n",
    "            \n",
    "                #Zero out all the detections that have IoU > treshhold\n",
    "                iou_mask = (ious < nms_conf).float().unsqueeze(1)\n",
    "                image_pred_class[i+1:] *= iou_mask       \n",
    "            \n",
    "                #Remove the non-zero entries\n",
    "                non_zero_ind = torch.nonzero(image_pred_class[:,4]).squeeze()\n",
    "                image_pred_class = image_pred_class[non_zero_ind].view(-1,7)\n",
    "                \n",
    "            batch_ind = image_pred_class.new(image_pred_class.size(0), 1).fill_(ind)      #Repeat the batch_id for as many detections of the class cls in the image\n",
    "            seq = batch_ind, image_pred_class\n",
    "            \n",
    "            if not write:\n",
    "                output = torch.cat(seq,1)\n",
    "                write = True\n",
    "            else:\n",
    "                out = torch.cat(seq,1)\n",
    "                output = torch.cat((output,out))\n",
    "\n",
    "    try:\n",
    "        return output\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "def letterbox_image(img, inp_dim):\n",
    "    '''resize image with unchanged aspect ratio using padding'''\n",
    "    img_w, img_h = img.shape[1], img.shape[0]\n",
    "    w, h = inp_dim\n",
    "    new_w = int(img_w * min(w/img_w, h/img_h))\n",
    "    new_h = int(img_h * min(w/img_w, h/img_h))\n",
    "    resized_image = cv2.resize(img, (new_w,new_h), interpolation = cv2.INTER_CUBIC)\n",
    "    \n",
    "    canvas = np.full((inp_dim[1], inp_dim[0], 3), 128)\n",
    "\n",
    "    canvas[(h-new_h)//2:(h-new_h)//2 + new_h,(w-new_w)//2:(w-new_w)//2 + new_w,  :] = resized_image\n",
    "    \n",
    "    return canvas\n",
    "\n",
    "def prep_image(img, inp_dim):\n",
    "    \"\"\"\n",
    "    Prepare image for inputting to the neural network. \n",
    "    \n",
    "    Returns a Variable \n",
    "    \"\"\"\n",
    "    # pylint: disable=no-member\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = letterbox_image(img, (inp_dim, inp_dim))\n",
    "    return torch.from_numpy(img.transpose(2, 0, 1)).float().div(255.0).unsqueeze(0)\n",
    "\n",
    "def load_classes(namesfile):\n",
    "    fp = open(namesfile, \"r\")\n",
    "    names = fp.read().split(\"\\n\")[:-1]\n",
    "    return names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# darknet.py\n",
    "from __future__ import division\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def get_test_input():\n",
    "    img = cv2.imread(\"dog-cycle-car.png\")\n",
    "    img = cv2.resize(img, (608,608))          #Resize to the input dimension\n",
    "    img_ =  img[:,:,::-1].transpose((2,0,1))  # BGR -> RGB | H X W C -> C X H X W \n",
    "    img_ = img_[np.newaxis,:,:,:]/255.0       #Add a channel at 0 (for batch) | Normalise\n",
    "    img_ = torch.from_numpy(img_).float()     #Convert to float\n",
    "    img_ = Variable(img_)                     # Convert to Variable\n",
    "    return img_\n",
    "\n",
    "def parse_cfg(cfgfile):\n",
    "    \"\"\"\n",
    "    Takes a configuration file\n",
    "    \n",
    "    Returns a list of blocks. Each blocks describes a block in the neural\n",
    "    network to be built. Block is represented as a dictionary in the list\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    file = open(cfgfile, 'r')\n",
    "    lines = file.read().split('\\n')                        # store the lines in a list\n",
    "    lines = [x for x in lines if len(x) > 0]               # get read of the empty lines \n",
    "    lines = [x for x in lines if x[0] != '#']              # get rid of comments\n",
    "    lines = [x.rstrip().lstrip() for x in lines]           # get rid of fringe whitespaces\n",
    "    \n",
    "    block = {}\n",
    "    blocks = []\n",
    "    \n",
    "    for line in lines:\n",
    "        if line[0] == \"[\":               # This marks the start of a new block\n",
    "            if len(block) != 0:          # If block is not empty, implies it is storing values of previous block.\n",
    "                blocks.append(block)     # add it the blocks list\n",
    "                block = {}               # re-init the block\n",
    "            block[\"type\"] = line[1:-1].rstrip()     \n",
    "        else:\n",
    "            key,value = line.split(\"=\") \n",
    "            block[key.rstrip()] = value.lstrip()\n",
    "    blocks.append(block)\n",
    "    \n",
    "    return blocks\n",
    "\n",
    "\n",
    "class EmptyLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmptyLayer, self).__init__()\n",
    "        \n",
    "\n",
    "class DetectionLayer(nn.Module):\n",
    "    def __init__(self, anchors):\n",
    "        super(DetectionLayer, self).__init__()\n",
    "        self.anchors = anchors\n",
    "\n",
    "class Mish(nn.Module):\n",
    "     def __init__(self):\n",
    "         super().__init__()\n",
    "\n",
    "     def forward(self, x):\n",
    "         return x * torch.tanh(F.softplus(x))\n",
    "\n",
    "\n",
    "\n",
    "def create_modules(blocks):\n",
    "    net_info = blocks[0]     #Captures the information about the input and pre-processing    \n",
    "    module_list = nn.ModuleList()\n",
    "    prev_filters = 3\n",
    "    output_filters = []\n",
    "    \n",
    "    for index, x in enumerate(blocks[1:]):\n",
    "        module = nn.Sequential()\n",
    "    \n",
    "        #check the type of block\n",
    "        #create a new module for the block\n",
    "        #append to module_list\n",
    "        \n",
    "        #If it's a convolutional layer\n",
    "        if (x[\"type\"] == \"convolutional\"):\n",
    "            #Get the info about the layer\n",
    "            activation = x[\"activation\"]\n",
    "            try:\n",
    "                batch_normalize = int(x[\"batch_normalize\"])\n",
    "                bias = False\n",
    "            except:\n",
    "                batch_normalize = 0\n",
    "                bias = True\n",
    "        \n",
    "            filters= int(x[\"filters\"])\n",
    "            padding = int(x[\"pad\"])\n",
    "            kernel_size = int(x[\"size\"])\n",
    "            stride = int(x[\"stride\"])\n",
    "        \n",
    "            if padding:\n",
    "                pad = (kernel_size - 1) // 2\n",
    "            else:\n",
    "                pad = 0\n",
    "        \n",
    "            #Add the convolutional layer\n",
    "            conv = nn.Conv2d(prev_filters, filters, kernel_size, stride, pad, bias = bias)\n",
    "            module.add_module(\"conv_{0}\".format(index), conv)\n",
    "        \n",
    "            #Add the Batch Norm Layer\n",
    "            if batch_normalize:\n",
    "                bn = nn.BatchNorm2d(filters)\n",
    "                module.add_module(\"batch_norm_{0}\".format(index), bn)\n",
    "        \n",
    "            #Check the activation. \n",
    "            #It is either Linear or a Leaky ReLU for YOLO\n",
    "            if activation == \"leaky\":\n",
    "                activn = nn.LeakyReLU(0.1, inplace = True)\n",
    "                module.add_module(\"leaky_{0}\".format(index), activn)\n",
    "\n",
    "            if activation == \"mish\":\n",
    "                activn = Mish()\n",
    "                module.add_module(\"mish_{0}\".format(index), activn)\n",
    "        \n",
    "            #If it's an upsampling layer\n",
    "            #We use Bilinear2dUpsampling\n",
    "        elif (x[\"type\"] == \"upsample\"):\n",
    "            stride = int(x[\"stride\"])\n",
    "            upsample = nn.Upsample(scale_factor = 2, mode = \"nearest\")\n",
    "            module.add_module(\"upsample_{}\".format(index), upsample)\n",
    "                \n",
    "        #If it is a route layer\n",
    "        elif (x[\"type\"] == \"route\"):\n",
    "            x[\"layers\"] = x[\"layers\"].split(',')\n",
    "            #Start  of a route\n",
    "            start = int(x[\"layers\"][0])\n",
    "            #end, if there exists one.\n",
    "            try:\n",
    "                end = int(x[\"layers\"][1])\n",
    "            except:\n",
    "                end = 0\n",
    "            #Positive anotation\n",
    "            if start > 0: \n",
    "                start = start - index\n",
    "            if end > 0:\n",
    "                end = end - index\n",
    "            route = EmptyLayer()\n",
    "            module.add_module(\"route_{0}\".format(index), route)\n",
    "            if end < 0:\n",
    "                filters = output_filters[index + start] + output_filters[index + end]\n",
    "            else:\n",
    "                filters= output_filters[index + start]\n",
    "    \n",
    "        #shortcut corresponds to skip connection\n",
    "        elif x[\"type\"] == \"shortcut\":\n",
    "            shortcut = EmptyLayer()\n",
    "            module.add_module(\"shortcut_{}\".format(index), shortcut)\n",
    "            \n",
    "        #Yolo is the detection layer\n",
    "        elif x[\"type\"] == \"yolo\":\n",
    "            mask = x[\"mask\"].split(\",\")\n",
    "            mask = [int(x) for x in mask]\n",
    "    \n",
    "            anchors = x[\"anchors\"].split(\",\")\n",
    "            anchors = [int(a) for a in anchors]\n",
    "            anchors = [(anchors[i], anchors[i+1]) for i in range(0, len(anchors),2)]\n",
    "            anchors = [anchors[i] for i in mask]\n",
    "    \n",
    "            detection = DetectionLayer(anchors)\n",
    "            module.add_module(\"Detection_{}\".format(index), detection)\n",
    "        \n",
    "        elif x[\"type\"] == \"maxpool\":\n",
    "            stride = int(x[\"stride\"])\n",
    "            size = int(x[\"size\"])\n",
    "            assert size % 2\n",
    "            maxpool = nn.MaxPool2d(kernel_size=size, stride=stride, padding=size // 2)\n",
    "            module.add_module(\"maxpool_{0}\".format(index), maxpool)\n",
    "                              \n",
    "        module_list.append(module)\n",
    "        prev_filters = filters\n",
    "        output_filters.append(filters)\n",
    "        \n",
    "    return (net_info, module_list)\n",
    "\n",
    "class Darknet(nn.Module):\n",
    "    def __init__(self, cfgfile):\n",
    "        super(Darknet, self).__init__()\n",
    "        self.blocks = parse_cfg(cfgfile)\n",
    "        self.net_info, self.module_list = create_modules(self.blocks)\n",
    "        \n",
    "    def forward(self, x, CUDA):\n",
    "        modules = self.blocks[1:]\n",
    "        outputs = {}   #We cache the outputs for the route layer\n",
    "        \n",
    "        write = 0\n",
    "        for i, module in enumerate(modules):        \n",
    "            module_type = (module[\"type\"])\n",
    "            \n",
    "            if module_type == \"convolutional\" or module_type == \"upsample\" or module_type == \"maxpool\":\n",
    "                x = self.module_list[i](x)\n",
    "                \n",
    "    \n",
    "            elif module_type == \"route\":\n",
    "                \n",
    "                # concat layers\n",
    "                layers = module[\"layers\"]\n",
    "                layers = [int(a) for a in layers]\n",
    "                \n",
    "                if (layers[0]) > 0:\n",
    "                    layers[0] = layers[0] - i\n",
    "    \n",
    "                if len(layers) == 1:    # 1 item in layer                 \n",
    "                    x = outputs[i + (layers[0])]\n",
    "    \n",
    "                else:   # more than 1 item in layer \n",
    "                    if len(layers) == 4:       # 4 items in layer                                      \n",
    "                        if (layers[1]) > 0:\n",
    "                            layers[1] = layers[1] - i\n",
    "\n",
    "                        if (layers[2]) > 0:\n",
    "                            layers[2] = layers[2] - i\n",
    "\n",
    "                        if (layers[3]) > 0:\n",
    "                            layers[3] = layers[3] - i\n",
    "\n",
    "                        map1 = outputs[i + layers[0]]\n",
    "                        map2 = outputs[i + layers[1]]\n",
    "                        map3 = outputs[i + layers[2]]\n",
    "                        map4 = outputs[i + layers[3]]\n",
    "                        x = torch.cat((map1, map2, map3, map4), 1)\n",
    "\n",
    "                    else:           # 2 items in layer                \n",
    "                        if (layers[1]) > 0:\n",
    "                            layers[1] = layers[1] - i\n",
    "        \n",
    "                        map1 = outputs[i + layers[0]]\n",
    "                        map2 = outputs[i + layers[1]]\n",
    "                        x = torch.cat((map1, map2), 1)\n",
    "                \n",
    "                \n",
    "            elif module_type == \"shortcut\":\n",
    "                from_ = int(module[\"from\"])\n",
    "                x = outputs[i-1] + outputs[i+from_]\n",
    "                \n",
    "    \n",
    "            elif module_type == 'yolo':        \n",
    "                anchors = self.module_list[i][0].anchors\n",
    "                #Get the input dimensions\n",
    "                inp_dim = int (self.net_info[\"height\"])\n",
    "        \n",
    "                #Get the number of classes\n",
    "                num_classes = int (module[\"classes\"])\n",
    "        \n",
    "                #Transform \n",
    "                x = x.data\n",
    "                x = predict_transform(x, inp_dim, anchors, num_classes, CUDA)\n",
    "                if not write:              #if no collector has been intialised. \n",
    "                    detections = x\n",
    "                    write = 1\n",
    "        \n",
    "                else:       \n",
    "                    detections = torch.cat((detections, x), 1)\n",
    "                \n",
    "            \n",
    "            outputs[i] = x\n",
    "        \n",
    "        return detections\n",
    "\n",
    "\n",
    "    def load_weights(self, weightfile):\n",
    "        #Open the weights file\n",
    "        fp = open(weightfile, \"rb\")\n",
    "    \n",
    "        #The first 5 values are header information \n",
    "        # 1. Major version number\n",
    "        # 2. Minor Version Number\n",
    "        # 3. Subversion number \n",
    "        # 4,5. Images seen by the network (during training)\n",
    "        header = np.fromfile(fp, dtype = np.int32, count = 5)\n",
    "        self.header = torch.from_numpy(header)\n",
    "        self.seen = self.header[3]   \n",
    "        \n",
    "        weights = np.fromfile(fp, dtype = np.float32)\n",
    "        \n",
    "        ptr = 0\n",
    "        for i in range(len(self.module_list)):\n",
    "            module_type = self.blocks[i + 1][\"type\"]\n",
    "    \n",
    "            #If module_type is convolutional load weights\n",
    "            #Otherwise ignore.\n",
    "            \n",
    "            if module_type == \"convolutional\":\n",
    "                model = self.module_list[i]\n",
    "                try:\n",
    "                    batch_normalize = int(self.blocks[i+1][\"batch_normalize\"])\n",
    "                except:\n",
    "                    batch_normalize = 0\n",
    "            \n",
    "                conv = model[0]\n",
    "                \n",
    "                \n",
    "                if (batch_normalize):\n",
    "                    bn = model[1]\n",
    "        \n",
    "                    #Get the number of weights of Batch Norm Layer\n",
    "                    num_bn_biases = bn.bias.numel()\n",
    "        \n",
    "                    #Load the weights\n",
    "                    bn_biases = torch.from_numpy(weights[ptr:ptr + num_bn_biases])\n",
    "                    ptr += num_bn_biases\n",
    "        \n",
    "                    bn_weights = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n",
    "                    ptr  += num_bn_biases\n",
    "        \n",
    "                    bn_running_mean = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n",
    "                    ptr  += num_bn_biases\n",
    "        \n",
    "                    bn_running_var = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n",
    "                    ptr  += num_bn_biases\n",
    "        \n",
    "                    #Cast the loaded weights into dims of model weights. \n",
    "                    bn_biases = bn_biases.view_as(bn.bias.data)\n",
    "                    bn_weights = bn_weights.view_as(bn.weight.data)\n",
    "                    bn_running_mean = bn_running_mean.view_as(bn.running_mean)\n",
    "                    bn_running_var = bn_running_var.view_as(bn.running_var)\n",
    "        \n",
    "                    #Copy the data to model\n",
    "                    bn.bias.data.copy_(bn_biases)\n",
    "                    bn.weight.data.copy_(bn_weights)\n",
    "                    bn.running_mean.copy_(bn_running_mean)\n",
    "                    bn.running_var.copy_(bn_running_var)\n",
    "                \n",
    "                else:\n",
    "                    #Number of biases\n",
    "                    num_biases = conv.bias.numel()\n",
    "                \n",
    "                    #Load the weights\n",
    "                    conv_biases = torch.from_numpy(weights[ptr: ptr + num_biases])\n",
    "                    ptr = ptr + num_biases\n",
    "                \n",
    "                    #reshape the loaded weights according to the dims of the model weights\n",
    "                    conv_biases = conv_biases.view_as(conv.bias.data)\n",
    "                \n",
    "                    #Finally copy the data\n",
    "                    conv.bias.data.copy_(conv_biases)\n",
    "                    \n",
    "                #Let us load the weights for the Convolutional layers\n",
    "                num_weights = conv.weight.numel()\n",
    "                \n",
    "                #Do the same as above for weights\n",
    "                conv_weights = torch.from_numpy(weights[ptr:ptr+num_weights])\n",
    "                ptr = ptr + num_weights\n",
    "                \n",
    "                conv_weights = conv_weights.view_as(conv.weight.data)\n",
    "                conv.weight.data.copy_(conv_weights)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading network.....\n",
      "Network successfully loaded\n",
      "ait-orientation.jpg  predicted in  0.008 seconds\n",
      "Objects Detected:    person person person person person handbag handbag suitcase suitcase suitcase\n",
      "----------------------------------------------------------\n",
      "SUMMARY\n",
      "----------------------------------------------------------\n",
      "Task                     : Time Taken (in seconds)\n",
      "\n",
      "Reading addresses        : 0.000\n",
      "Loading batch            : 0.100\n",
      "Detection (1 images)     : 0.034\n",
      "Output Processing        : 0.000\n",
      "Drawing Boxes            : 0.086\n",
      "Average time_per_img     : 0.220\n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Code to load model, image, and display result here\n",
    "from __future__ import division\n",
    "import time\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import cv2 \n",
    "import argparse\n",
    "import os \n",
    "import os.path as osp\n",
    "# from darknet import Darknet\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "images = \"ait-orientation.jpg\"\n",
    "batch_size = 4\n",
    "confidence = 0.5\n",
    "nms_thesh = 0.4\n",
    "start = 0\n",
    "CUDA = torch.cuda.is_available()\n",
    "\n",
    "num_classes = 80\n",
    "classes = load_classes(\"coco.names\")\n",
    "\n",
    "#Set up the neural network\n",
    "\n",
    "print(\"Loading network.....\")\n",
    "model = Darknet(\"cfg/yolov4.cfg\")\n",
    "\n",
    "# Edit Convo Layer 114\n",
    "# Here we need to edit this layer because previously the input channel to this was set as 1024 but actually this layer needs to accept the input from the concatenation of four 512-channel layers so I need to modify this layer to have input channel of 2048\n",
    "model.module_list[114].conv_114 = nn.Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "\n",
    "model.load_weights(\"yolov4.weights\")\n",
    "print(\"Network successfully loaded\")\n",
    "\n",
    "model.net_info[\"height\"] = 416\n",
    "inp_dim = int(model.net_info[\"height\"])\n",
    "assert inp_dim % 32 == 0 \n",
    "assert inp_dim > 32\n",
    "\n",
    "#If there's a GPU availible, put the model on GPU\n",
    "\n",
    "if CUDA:\n",
    "    model.cuda()\n",
    "\n",
    "# Set the model in evaluation mode\n",
    "\n",
    "model.eval()\n",
    "\n",
    "read_dir = time.time()\n",
    "\n",
    "# Detection phase\n",
    "\n",
    "try:\n",
    "    imlist = [osp.join(osp.realpath('.'), images, img) for img in os.listdir(images)]\n",
    "except NotADirectoryError:\n",
    "    imlist = []\n",
    "    imlist.append(osp.join(osp.realpath('.'), images))\n",
    "except FileNotFoundError:\n",
    "    print (\"No file or directory with the name {}\".format(images))\n",
    "    exit()\n",
    "    \n",
    "if not os.path.exists(\"des\"):\n",
    "    os.makedirs(\"des\")\n",
    "\n",
    "load_batch = time.time()\n",
    "loaded_ims = [cv2.imread(x) for x in imlist]\n",
    "\n",
    "im_batches = list(map(prep_image, loaded_ims, [inp_dim for x in range(len(imlist))]))\n",
    "im_dim_list = [(x.shape[1], x.shape[0]) for x in loaded_ims]\n",
    "im_dim_list = torch.FloatTensor(im_dim_list).repeat(1,2)\n",
    "\n",
    "\n",
    "leftover = 0\n",
    "if (len(im_dim_list) % batch_size):\n",
    "    leftover = 1\n",
    "\n",
    "if batch_size != 1:\n",
    "    num_batches = len(imlist) // batch_size + leftover            \n",
    "    im_batches = [torch.cat((im_batches[i*batch_size : min((i +  1)*batch_size,\n",
    "                        len(im_batches))]))  for i in range(num_batches)]  \n",
    "\n",
    "write = 0\n",
    "\n",
    "if CUDA:\n",
    "    im_dim_list = im_dim_list.cuda()\n",
    "    \n",
    "start_det_loop = time.time()\n",
    "for i, batch in enumerate(im_batches):\n",
    "    # Load the image \n",
    "    start = time.time()\n",
    "    if CUDA:\n",
    "        batch = batch.cuda()\n",
    "    with torch.no_grad():\n",
    "        prediction = model(Variable(batch), CUDA)\n",
    "\n",
    "    prediction = write_results(prediction, confidence, num_classes, nms_conf = nms_thesh)\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    if type(prediction) == int:\n",
    "\n",
    "        for im_num, image in enumerate(imlist[i*batch_size: min((i +  1)*batch_size, len(imlist))]):\n",
    "            im_id = i*batch_size + im_num\n",
    "            print(\"{0:20s} predicted in {1:6.3f} seconds\".format(image.split(\"/\")[-1], (end - start)/batch_size))\n",
    "            print(\"{0:20s} {1:s}\".format(\"Objects Detected:\", \"\"))\n",
    "            print(\"----------------------------------------------------------\")\n",
    "        continue\n",
    "\n",
    "    prediction[:,0] += i*batch_size    #transform the atribute from index in batch to index in imlist \n",
    "\n",
    "    if not write:                      #If we have't initialised output\n",
    "        output = prediction  \n",
    "        write = 1\n",
    "    else:\n",
    "        output = torch.cat((output,prediction))\n",
    "\n",
    "    for im_num, image in enumerate(imlist[i*batch_size: min((i +  1)*batch_size, len(imlist))]):\n",
    "        im_id = i*batch_size + im_num\n",
    "        objs = [classes[int(x[-1])] for x in output if int(x[0]) == im_id]\n",
    "        print(\"{0:20s} predicted in {1:6.3f} seconds\".format(image.split(\"/\")[-1], (end - start)/batch_size))\n",
    "        print(\"{0:20s} {1:s}\".format(\"Objects Detected:\", \" \".join(objs)))\n",
    "        print(\"----------------------------------------------------------\")\n",
    "\n",
    "    if CUDA:\n",
    "        torch.cuda.synchronize()       \n",
    "try:\n",
    "    output\n",
    "except NameError:\n",
    "    print (\"No detections were made\")\n",
    "    exit()\n",
    "\n",
    "im_dim_list = torch.index_select(im_dim_list, 0, output[:,0].long())\n",
    "\n",
    "scaling_factor = torch.min(416/im_dim_list,1)[0].view(-1,1)\n",
    "\n",
    "output[:,[1,3]] -= (inp_dim - scaling_factor*im_dim_list[:,0].view(-1,1))/2\n",
    "output[:,[2,4]] -= (inp_dim - scaling_factor*im_dim_list[:,1].view(-1,1))/2\n",
    "\n",
    "output[:,1:5] /= scaling_factor\n",
    "\n",
    "for i in range(output.shape[0]):\n",
    "    output[i, [1,3]] = torch.clamp(output[i, [1,3]], 0.0, im_dim_list[i,0])\n",
    "    output[i, [2,4]] = torch.clamp(output[i, [2,4]], 0.0, im_dim_list[i,1])\n",
    "    \n",
    "output_recast = time.time()\n",
    "class_load = time.time()\n",
    "colors = [[255, 0, 0], [255, 0, 0], [255, 255, 0], [0, 255, 0], [0, 255, 255], [0, 0, 255], [255, 0, 255]]\n",
    "\n",
    "draw = time.time()\n",
    "\n",
    "def write(x, results):\n",
    "    c1 = x[1:3].int().detach().cpu().numpy()\n",
    "    c2 = x[3:5].int().detach().cpu().numpy()\n",
    "    img = results[int(x[0])]\n",
    "    cls = int(x[-1])\n",
    "    color = random.choice(colors)\n",
    "    label = \"{0}\".format(classes[cls])\n",
    "    cv2.rectangle(img, c1, c2,color, 1)\n",
    "    t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1 , 1)[0]\n",
    "    c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4\n",
    "    cv2.rectangle(img, c1, c2,color, -1)\n",
    "    cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1, [225,255,255], 1);\n",
    "    return img\n",
    "\n",
    "\n",
    "list(map(lambda x: write(x, loaded_ims), output))\n",
    "\n",
    "det_names = pd.Series(imlist).apply(lambda x: \"{}/det_{}\".format(\"des\",x.split(\"/\")[-1]))\n",
    "\n",
    "list(map(cv2.imwrite, det_names, loaded_ims))\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"SUMMARY\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"{:25s}: {}\".format(\"Task\", \"Time Taken (in seconds)\"))\n",
    "print()\n",
    "print(\"{:25s}: {:2.3f}\".format(\"Reading addresses\", load_batch - read_dir))\n",
    "print(\"{:25s}: {:2.3f}\".format(\"Loading batch\", start_det_loop - load_batch))\n",
    "print(\"{:25s}: {:2.3f}\".format(\"Detection (\" + str(len(imlist)) +  \" images)\", output_recast - start_det_loop))\n",
    "print(\"{:25s}: {:2.3f}\".format(\"Output Processing\", class_load - output_recast))\n",
    "print(\"{:25s}: {:2.3f}\".format(\"Drawing Boxes\", end - draw))\n",
    "print(\"{:25s}: {:2.3f}\".format(\"Average time_per_img\", (end - load_batch)/len(imlist)))\n",
    "print(\"----------------------------------------------------------\")\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](des/det_ait-orientation.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (10 points)\n",
    "\n",
    "In Labs 02-03, you became familiar with different image classification models and the\n",
    "technique of retraining/fine-tuning a pre-trained model on a new dataset. Let's create\n",
    "a ResNet model for classifying images in the CIFAR100 dataset.\n",
    "   \n",
    "First, create dataset objects for the CIFAR100 training and test sets. You'll find\n",
    "documentation at [the torchvision datasets page](https://pytorch.org/vision/stable/datasets.html).\n",
    "To use the already-downloaded dataset on puffer/gourami/guppy, use the following dataset location:\n",
    "\n",
    "    train_dataset = torchvision.datasets.CIFAR100('/home/fidji/mdailey/Datasets/CIFAR100', train=True)\n",
    "\n",
    "Write some code to get one of the samples from the dataset object. Show your code here, and display\n",
    "the image print its attributes here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to extract a sample from the dataset\n",
    "# Import libraries\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import os\n",
    "from copy import copy\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from torchvision.transforms.transforms import RandomCrop\n",
    "from torchvision.datasets.utils import download_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "## Resize to 256\n",
    "### AUGMENT for train\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "## Resize to 224\n",
    "### No AUGMENT for test\n",
    "eval_preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "# Download CIFAR-10 and split into training, validation, and test sets.\n",
    "# The copy of the training dataset after the split allows us to keep\n",
    "# the same training/validation split of the original training set but\n",
    "# apply different transforms to the training set and validation set.\n",
    "\n",
    "full_train_dataset = torchvision.datasets.CIFAR100('./MyCIFAR', train=True, download=True)\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(full_train_dataset, [40000, 10000])\n",
    "train_dataset.dataset = copy(full_train_dataset)\n",
    "train_dataset.dataset.transform = train_preprocess\n",
    "val_dataset.dataset.transform = eval_preprocess\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR100('./MyCIFAR', train=False,\n",
    "                                            download=True, transform=eval_preprocess)\n",
    "\n",
    "# train_dataset = torchvision.datasets.CIFAR100('./MyCIFAR', train=True, download=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Show your sample image and its attributes here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (20 points)\n",
    "\n",
    "Next, create data loaders for the training dataset and validation dataset (no need to use the test set).\n",
    "Use a batch size of 4 and appropriate transforms for the training and validation sets.\n",
    "\n",
    "Put your code to create the data loaders, sample one minibatch from the training set, and output the\n",
    "shapes of the tensors comprising the minibatch here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to create dataloaders, sample a minibatch, and print out tensor shape here\n",
    "\n",
    "# Prepare the data loaders\n",
    "BATCH_SIZE=4\n",
    "NUM_WORKERS=2\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                                            shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
    "                                            shuffle=False, num_workers=NUM_WORKERS)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                                            shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "dataloaders = {'train': train_dataloader, 'val': val_dataloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseball glove baseball glove keyboard oven \n"
     ]
    }
   ],
   "source": [
    "# full_train_dataset.classes\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     \n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_dataloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 (20 points)\n",
    "\n",
    "Next, create a ResNet-50 model with pretrained weights from ImageNet using the [torchvision ResNet class](https://pytorch.org/vision/stable/models.html#id10).\n",
    "Remove the classification layer and replace it with a layer appropriate for identification in CIFAR100. Show that your resulting model can process a minibatch\n",
    "from your validation dataloader and output (incorrect) identities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to create a ResNet 50 model, remove classification layer, replace with a CIFAR100 identity layer, and run in evaluation model on a validation minibatch\n",
    "#%% Blottleneck Block\n",
    "class BottleneckBlock(nn.Module):\n",
    "    '''\n",
    "    BottleneckBlock: More powerful residual block with three convs, used for Resnet50 and up\n",
    "    '''\n",
    "    EXPANSION = 4\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.EXPANSION * planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.EXPANSION * planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        # If the output size is not equal to input size, reshape it with 1x1 convolution\n",
    "        if stride != 1 or in_planes != self.EXPANSION * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.EXPANSION * planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.EXPANSION * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super().__init__()\n",
    "#         self.is_debug = False\n",
    "        self.in_planes = 64\n",
    "        # Initial convolution\n",
    "#         self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.maxpool = nn.MaxPool2d(3, stride = 2, padding = 1)\n",
    "        # Residual blocks\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        # FC layer = 1 layer\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.linear = nn.Linear(512 * block.EXPANSION, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.EXPANSION\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "#         if self.is_debug : print(f'conv1: {out.shape}')\n",
    "        out = self.maxpool(out)\n",
    "#         if self.is_debug : print(f'max pool: {out.shape}')\n",
    "        out = self.layer1(out)\n",
    "#         if self.is_debug : print(f'conv2_x: {out.shape}')\n",
    "        out = self.layer2(out)\n",
    "#         if self.is_debug : print(f'conv3_x: {out.shape}')\n",
    "        out = self.layer3(out)\n",
    "#         if self.is_debug : print(f'conv4_x: {out.shape}')\n",
    "        out = self.layer4(out)\n",
    "#         if self.is_debug : print(f'conv5_x: {out.shape}')\n",
    "        out = self.avgpool(out)\n",
    "#         if self.is_debug : print(f'avg pool: {out.shape}')\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "    \n",
    "def ResNet50(num_classes = 10):\n",
    "    '''\n",
    "    First conv layer: 1\n",
    "    4 residual blocks with [3, 4, 6, 3] sets of three convolutions each: 3*3 + 4*3 + 6*3 + 3*3 = 48\n",
    "    last FC layer: 1\n",
    "    Total layers: 1+48+1 = 50\n",
    "    '''\n",
    "    return ResNet(BottleneckBlock, [3, 4, 6, 3], num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 (20 points)\n",
    "\n",
    "Next, write a training function, create an optimizer and loss function, and show training loss and validation loss for one epoch.\n",
    "\n",
    "Show the new ouptut identities for the validation minibatch used in Question 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/0\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:247: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:247: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Loss.cu:247: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 287>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;66;03m# Now we'll use Adam optimization\u001b[39;00m\n\u001b[1;32m    285\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(params_to_update, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m--> 287\u001b[0m best_model, val_acc_history, loss_acc_history \u001b[38;5;241m=\u001b[39m train_model(resnet, dataloaders, criterion, optimizer, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresnet50_bestsofar\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloaders, criterion, optimizer, num_epochs, weights_name, is_inception)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;66;03m# backward + optimize only if in training phase\u001b[39;00m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m phase \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 206\u001b[0m         \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# statistics\u001b[39;00m\n",
      "File \u001b[0;32m~/pythonDSAI/lib/python3.8/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pythonDSAI/lib/python3.8/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`"
     ]
    }
   ],
   "source": [
    "# # Code for training, optimizer, loss here, training one epoch, and new result on validation minibatch\n",
    "\n",
    "# def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, weights_name='weight_save', is_inception=False):\n",
    "#     print('====================== New Run =======================',file=open(\"output.txt\", \"a\"))\n",
    "#     '''\n",
    "#     train_model: train a model on a dataset\n",
    "    \n",
    "#             Parameters:\n",
    "#                     model: Pytorch model\n",
    "#                     dataloaders: dataset\n",
    "#                     criterion: loss function\n",
    "#                     optimizer: update weights function\n",
    "#                     num_epochs: number of epochs\n",
    "#                     weights_name: file name to save weights\n",
    "#                     is_inception: The model is inception net (Google LeNet) or not\n",
    "\n",
    "#             Returns:\n",
    "#                     model: Best model from evaluation result\n",
    "#                     val_acc_history: evaluation accuracy history\n",
    "#                     loss_acc_history: loss value history\n",
    "#     '''\n",
    "#     since = time.time()\n",
    "\n",
    "#     val_acc_history = []\n",
    "#     loss_acc_history = []\n",
    "\n",
    "#     best_model_wts = deepcopy(model.state_dict())\n",
    "#     best_acc = 0.0\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         epoch_start = time.time()\n",
    "#         print('Epoch {}/{}'.format(epoch, num_epochs - 1), file=open(\"output.txt\", \"a\"))\n",
    "#         print('-' * 10, file=open(\"output.txt\", \"a\"))\n",
    "\n",
    "#         # Each epoch has a training and validation phase\n",
    "#         for phase in ['train', 'val']:\n",
    "#             if phase == 'train':\n",
    "#                 model.train()  # Set model to training mode\n",
    "#             else:\n",
    "#                 model.eval()   # Set model to evaluate mode\n",
    "\n",
    "#             running_loss = 0.0\n",
    "#             running_corrects = 0\n",
    "\n",
    "#             # Iterate over data.\n",
    "#             for inputs, labels in dataloaders[phase]:\n",
    "#                 # for process anything, device and dataset must put in the same place.\n",
    "#                 # If the model is in GPU, input and output must set to GPU\n",
    "#                 inputs = inputs.to(device)\n",
    "#                 labels = labels.to(device)\n",
    "\n",
    "#                 # zero the parameter gradients\n",
    "#                 # it uses for update training weights\n",
    "#                 optimizer.zero_grad()\n",
    "\n",
    "#                 # forward\n",
    "#                 # track history if only in train\n",
    "#                 with torch.set_grad_enabled(phase == 'train'):\n",
    "#                     # Get model outputs and calculate loss\n",
    "#                     # Special case for inception because in training it has an auxiliary output. In train\n",
    "#                     #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "#                     #   but in testing we only consider the final output.\n",
    "#                     if is_inception and phase == 'train':\n",
    "#                         # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "#                         outputs, aux_outputs = model(inputs)\n",
    "#                         # print('outputs', outputs)\n",
    "#                         loss1 = criterion(outputs, labels)\n",
    "#                         loss2 = criterion(aux_outputs, labels)\n",
    "#                         loss = loss1 + 0.4*loss2\n",
    "#                     else:\n",
    "#                         outputs = model(inputs)\n",
    "#                         loss = criterion(outputs, labels)\n",
    "\n",
    "#                     _, preds = torch.max(outputs, 1)\n",
    "\n",
    "#                     # backward + optimize only if in training phase\n",
    "#                     if phase == 'train':\n",
    "#                         loss.backward()\n",
    "#                         optimizer.step()\n",
    "\n",
    "#                 # statistics\n",
    "#                 running_loss += loss.item() * inputs.size(0)\n",
    "#                 running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "#             epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "#             epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "#             epoch_end = time.time()\n",
    "            \n",
    "#             elapsed_epoch = epoch_end - epoch_start\n",
    "\n",
    "#             print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc), file=open(\"output.txt\", \"a\"))\n",
    "#             print(f\"Epoch time taken: {elapsed_epoch}\", file=open(\"output.txt\", \"a\"))\n",
    "\n",
    "#             # deep copy the model\n",
    "#             if phase == 'val' and epoch_acc > best_acc:\n",
    "#                 best_acc = epoch_acc\n",
    "#                 best_model_wts = deepcopy(model.state_dict())\n",
    "#                 torch.save(model.state_dict(), weights_name + \".pth\")\n",
    "#             if phase == 'val':\n",
    "#                 val_acc_history.append(epoch_acc)\n",
    "#             if phase == 'train':\n",
    "#                 loss_acc_history.append(epoch_loss)\n",
    "\n",
    "#         print()\n",
    "\n",
    "#     time_elapsed = time.time() - since\n",
    "#     print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "#     print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "#     # load best model weights\n",
    "#     model.load_state_dict(best_model_wts)\n",
    "#     return model, val_acc_history, loss_acc_history\n",
    "\n",
    "# resnet = ResNet50().to(device)\n",
    "# # number of trainable parameters\n",
    "# def count_parameters(model):\n",
    "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# print(f'number of trainable parameters: {count_parameters(resnet)}')\n",
    "\n",
    "# # Optimizer and loss function\n",
    "# criterion = nn.CrossEntropyLoss().to(device)\n",
    "# params_to_update = resnet.parameters()\n",
    "# # Now we'll use Adam optimization\n",
    "# #optimizer = optim.SGD(params_to_update, lr=0.01, weight_decay = 1e-4, momentum = 0.9)\n",
    "# optimizer = optim.Adam(params_to_update, lr=0.01)\n",
    "# #%% Run\n",
    "# #resnet.is_debug = True\n",
    "# best_model, val_acc_history, loss_acc_history = train_model(resnet, dataloaders, criterion, optimizer, 5, 'resnet50_bestsofar')\n",
    "\n",
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, weights_name='weight_save', is_inception=False):\n",
    "    '''\n",
    "    train_model: train a model on a dataset\n",
    "    \n",
    "            Parameters:\n",
    "                    model: Pytorch model\n",
    "                    dataloaders: dataset\n",
    "                    criterion: loss function\n",
    "                    optimizer: update weights function\n",
    "                    num_epochs: number of epochs\n",
    "                    weights_name: file name to save weights\n",
    "                    is_inception: The model is inception net (Google LeNet) or not\n",
    "\n",
    "            Returns:\n",
    "                    model: Best model from evaluation result\n",
    "                    val_acc_history: evaluation accuracy history\n",
    "                    loss_acc_history: loss value history\n",
    "    '''\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "    loss_acc_history = []\n",
    "\n",
    "    best_model_wts = deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                # for process anything, device and dataset must put in the same place.\n",
    "                # If the model is in GPU, input and output must set to GPU\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                # it uses for update training weights\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        # print('outputs', outputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            epoch_end = time.time()\n",
    "            \n",
    "            elapsed_epoch = epoch_end - epoch_start\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            print(\"Epoch time taken: \", elapsed_epoch)\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = deepcopy(model.state_dict())\n",
    "                torch.save(model.state_dict(), weights_name + \".pth\")\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "            if phase == 'train':\n",
    "                loss_acc_history.append(epoch_loss)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history, loss_acc_history\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    predicteds = []\n",
    "    trues = []\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch, labels in iterator:\n",
    "            \n",
    "            #Move tensors to the configured device\n",
    "            batch = batch.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            predictions = model(batch.float())\n",
    "                \n",
    "            loss = criterion(predictions, labels.long())\n",
    "            \n",
    "            predictions = nn.functional.softmax(predictions, dim=1)            \n",
    "            _, predicted = torch.max(predictions.data, 1)  #returns max value, indices\n",
    "                       \n",
    "            predicteds.append(predicted)\n",
    "            trues.append(labels)            \n",
    "            total += labels.size(0)  #keep track of total\n",
    "            correct += (predicted == labels).sum().item()  #.item() give the raw number\n",
    "            acc = 100 * (correct / total)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator),predicteds, trues\n",
    "\n",
    "resnet = ResNet50().to(device)\n",
    "# Optimizer and loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "params_to_update = resnet.parameters()\n",
    "# Now we'll use Adam optimization\n",
    "optimizer = optim.Adam(params_to_update, lr=0.01)\n",
    "\n",
    "best_model, val_acc_history, loss_acc_history = train_model(resnet, dataloaders, criterion, optimizer, 1, 'resnet50_bestsofar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_resnet50 = open(\"output.txt\", \"r\")\n",
    "# print(results_resnet50.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6 (10 points)\n",
    "\n",
    "Explain how you could use the model you just created as a classifier model in a Control GAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Put your explanation here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonDSAI",
   "language": "python",
   "name": "pythondsai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
